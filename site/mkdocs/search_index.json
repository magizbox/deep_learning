{
    "docs": [
        {
            "location": "/", 
            "text": "Deep Learning\n\n\n\n\nModern Practical Deep Networks\n\n\n\n\nFeedforward Deep Networks\n\n\nRegularization\n\n\nOptimization for Training Deep Models\n\n\nConvolutional Networks\n\n\nSequence Modeling: Recurrent and Recursive Nets\n\n\nApplications\n\n\n\n\nDeep Learning Research\n\n\n\n\nStructured Probabilistic Models for Deep Learning\n\n\nMonte Carlo Methods\n\n\nLinear Factor Models and Auto-Encoders\n\n\nRepresentation Learning\n\n\nThe Manifold Perspective on Representation Learning\n\n\nConfronting the Partition Function\n\n\nApproximate Inference\n\n\nDeep Generative Models\n\n\n\n\nSlide\n\n\n\n\nAndrew Ng, Deep Learning, http://www.slideshare.net/mobile/ExtractConf/andrew-ng-chief-scientist-at-baidu\n\n\n\n\nCompare Deep Learning Framework\n\n\nhttps://github.com/zer0n/deepframeworks/blob/master/README.md\n\n\nDeep Learning Courses\n\n\nDeep Learning Course\n\n\nPapers\n\n\nAwesome Deep Learning Papers\n\n\nDeep Learning Q\nA\n\n\nDeep Learning Q\nA \n1\n\n\n\n\nWhat is an auto-encoder? Why do we \"auto-encode\"? Hint: it's really a misnomer.\n\n\nWhat is a Boltzmann Machine? Why a Boltzmann Machine?\n\n\nWhy do we use sigmoid for an output function? Why tanh? Why not cosine? Why any function in particular?\n\n\nWhy are CNNs used primarily in imaging and not so much other tasks?\n\n\nExplain backpropagation. Seriously. To the target audience described above.\n\n\nIs it OK to connect from a Layer 4 output back to a Layer 2 input?\n\n\n\n\nA data-scientist person recently put up a YouTube video explaining that the essential difference between a Neural Network and a Deep Learning network is that the former is trained from output back to input, while the latter is trained from input toward output. Do you agree? Explain.\n\n\n\n\n\n\nCan you derive the back-propagation and weights update?\n\n\n\n\nExtend the above question to non-trivial layers such as convolutional layers, pooling layers, etc.\n\n\nHow to implement dropout\n\n\nYour intuition when and why some tricks such as max pooling, ReLU, maxout, etc. work. There are no right answers but it helps to understand your thoughts and research experience.\n\n\nCan youabstract the forward, backward, update operations as matrix operations, to leverage BLAS and GPU?\n\n\n\n\nIntro to Deep Learning\n\n\n\n\nWhat is deep learning?\n\n\n\nAccording to wikipedia \n1\n\n\nDeep learning\n \u00a0is a branch of machine learning based on a set of algorithms that \nattempt to model high-level abstractions\n in data by \nusing model architectures\n, with \ncomplex structures\n or otherwise, \ncomposed of multiple non-linear transformations.\n\n\nDeep learning is part of a broader family of machine learning methods based on \nlearning representations\n of data.\n\nAn observation (e.g., an image) can be represented in many ways such as a vector of intensity values per pixel, or in a more abstract way as a set of edges, regions of particular shape, etc.. Some representations make it easier to learn tasks (e.g., face recognition or facial expression recognition) from examples.\n\nOne of the promises of deep learning is replacing \nhandcrafted features\n with efficient algorithms for unsupervised or semi-supervised feature learning and hierarchical feature extraction.\n\n\n\nTraditional Model vs Deep Learning\n\n\n\nIn \ntraditional models\n, we must extract features by hand, after that we train these features with some classifiers\n\n\n\n\nWith \ndeep learning\n, we can learn representation of objects as well as its classifiers.\n\n\n\n\nHierarchy of representations with increasing level of abstraction. Each stage is a kind of trainable feature transform.\n\n\nImage recognition\n\n\npixel \n edge \n texton \n motif \n part \n object\n\n\nText\n\n\ncharacter \n word \n word group \n clause \n sentence \n story\n\n\nSpeech\n\n\nsample \n spectral band \n sound \n ... \n phone \n phoneme \n word\n\n\nDemos and Applications\n\n\n\nYann Lecun with ImageNetOnline Learning Demo in his deep learning class \n2\n.\u00a0This program auto learn new object when he pointed camera to it\n\n\n\n\nVoice recognition systems like Apple Siri, Google Now and Windows Cortana all use deep learning \n3\n\n\n\n\nFacebook's DeepFace Software Can Match Faces With 97.25% Accuracy \n4\n\n\n\n\nUseful Resources\n\n\n\n\n\nIntroduction about Deep Learning, http://colah.github.io/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\nDeep Learning, wikipedia\n\n\n\n\n\n\n\nDeep Learning Course of Yann Lecu, Week 1\n\n\n\n\n\n\n\nMeet The Guy Who Helped Google Beat Apple's Siri\n\n\n\n\n\n\n\u00a0\nFacebook's DeepFace Software Can Match Faces With 97.25% Accuracy\n\n\n\n\n\n\n\n\n\n\nTensorFlow\n\n\n\n\nTensorFlow\u2122 \n2\n is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.\n\n\nGetting Started\n\n\nLinux\n, \nMac\n, \nWindows\n\n\nWindows \n1\n\n\n[code lang=\"shell\"]\ndocker-machine ssh default\ndocker run -it b.gcr.io/tensorflow/tensorflow\n[/code]\n\n\nResources\n\n\n\n\nTensorFlow Explained by Jeff Dean (\nvideo\n)\n\n\n\n\nArchitectures\n\n\nAccording to Yann Lecun \n1\n, there are three types of deep architectures: feed-forward, feed-back and bi-directional.\n\n\nFeed-Forwards\n\n\n\nMultilayer Neural Nets \n2\n\n\n\nA multilayer perceptron (MLP) is a feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs. A MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. Except for the input nodes, each node is a neuron (or processing element) with a nonlinear activation function.\n\n\n\n\ntask: any supervised learning pattern recognition process\n\n\nConvolutional Neural Nets \n3\n\n\n\nIn machine learning, a convolutional neural network (CNN, or ConvNet) is a type of feed-forward artificial neural network where the individual neurons are tiled in such a way that they respond to overlapping regions in the visual field.Convolutional networks were inspired by biological processes and are variations of multilayer perceptrons which are designed to use minimal amounts of preprocessing. They are widely used models for image and video recognition.\n\n\n\n\ntask:\u00a0Computer Vision\n\n\nFeed-Back\n\n\n\nStacked sparse coding\n\n\n\nDeconvolutional nets\n\n\n\nBi-Directional\n\n\n\nRecurrent neural network \n4\n\n\n\nA recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior.\n\n\n\n\nTask: \nWord Embeddings\n\n\nDeep Boltzmann\u00a0Network\n\n\n\n\n\nStacked auto-encoders \n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttp://www.slideshare.net/yandex/yann-le-cun\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Multilayer_perceptron\n\n\n\n\n\n\n\u00a0\nhttps://en.wikipedia.org/wiki/Convolutional_neural_network\n\n\n\n\n\n\n\nRecurrent neural network\n\n\n\n\n\n\n\nhttp://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning\n\n\nDavid Silver (Google DeepMind) - Deep Reinforcement Learning\n\n\nRecurrent Neural Networks\n\n\nA recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. This makes them applicable to tasks such as unsegmented connected handwriting recognition or speech recognition\n\n\n1. Applications \n1\n\n\n1.1 Sequence-to-sequence language translation \n2\n\n\n\n\n1.2 Generate image caption\n\n\n\n\n1.3 Translate videos to sentences\n\n\n\n\n2. Challenges\n\n\n\n\nBag of Words Meets Bags of Popcorn\n\n\n\n\n3. Tutorials\n\n\n\n\nAnyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN)\n\n\nThe Unreasonable Effectiveness of Recurrent Neural Networks\n\n\n\n\n4. Library\n\n\n\n\nneuraltalk2\n \n(realtime demo)[https://vimeo.com/146492001]\n\n\n\n\n\n\n\n\n\n\n\n\nRecurrent Neural Network\n\n\n\n\n\n\nGeneral Sequence Learning using Recurrent Neural Networks", 
            "title": "Home"
        }, 
        {
            "location": "/#deep-learning", 
            "text": "", 
            "title": "Deep Learning"
        }, 
        {
            "location": "/#modern-practical-deep-networks", 
            "text": "Feedforward Deep Networks  Regularization  Optimization for Training Deep Models  Convolutional Networks  Sequence Modeling: Recurrent and Recursive Nets  Applications", 
            "title": "Modern Practical Deep Networks"
        }, 
        {
            "location": "/#deep-learning-research", 
            "text": "Structured Probabilistic Models for Deep Learning  Monte Carlo Methods  Linear Factor Models and Auto-Encoders  Representation Learning  The Manifold Perspective on Representation Learning  Confronting the Partition Function  Approximate Inference  Deep Generative Models", 
            "title": "Deep Learning Research"
        }, 
        {
            "location": "/#slide", 
            "text": "Andrew Ng, Deep Learning, http://www.slideshare.net/mobile/ExtractConf/andrew-ng-chief-scientist-at-baidu   Compare Deep Learning Framework  https://github.com/zer0n/deepframeworks/blob/master/README.md", 
            "title": "Slide"
        }, 
        {
            "location": "/#deep-learning-courses", 
            "text": "Deep Learning Course", 
            "title": "Deep Learning Courses"
        }, 
        {
            "location": "/#papers", 
            "text": "Awesome Deep Learning Papers", 
            "title": "Papers"
        }, 
        {
            "location": "/#deep-learning-qa", 
            "text": "Deep Learning Q A  1   What is an auto-encoder? Why do we \"auto-encode\"? Hint: it's really a misnomer.  What is a Boltzmann Machine? Why a Boltzmann Machine?  Why do we use sigmoid for an output function? Why tanh? Why not cosine? Why any function in particular?  Why are CNNs used primarily in imaging and not so much other tasks?  Explain backpropagation. Seriously. To the target audience described above.  Is it OK to connect from a Layer 4 output back to a Layer 2 input?   A data-scientist person recently put up a YouTube video explaining that the essential difference between a Neural Network and a Deep Learning network is that the former is trained from output back to input, while the latter is trained from input toward output. Do you agree? Explain.    Can you derive the back-propagation and weights update?   Extend the above question to non-trivial layers such as convolutional layers, pooling layers, etc.  How to implement dropout  Your intuition when and why some tricks such as max pooling, ReLU, maxout, etc. work. There are no right answers but it helps to understand your thoughts and research experience.  Can youabstract the forward, backward, update operations as matrix operations, to leverage BLAS and GPU?", 
            "title": "Deep Learning Q&amp;A"
        }, 
        {
            "location": "/#intro-to-deep-learning", 
            "text": "", 
            "title": "Intro to Deep Learning"
        }, 
        {
            "location": "/#tensorflow", 
            "text": "TensorFlow\u2122  2  is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.", 
            "title": "TensorFlow"
        }, 
        {
            "location": "/#getting-started", 
            "text": "Linux ,  Mac ,  Windows  Windows  1  [code lang=\"shell\"]\ndocker-machine ssh default\ndocker run -it b.gcr.io/tensorflow/tensorflow\n[/code]", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#resources", 
            "text": "TensorFlow Explained by Jeff Dean ( video )", 
            "title": "Resources"
        }, 
        {
            "location": "/#architectures", 
            "text": "According to Yann Lecun  1 , there are three types of deep architectures: feed-forward, feed-back and bi-directional.", 
            "title": "Architectures"
        }, 
        {
            "location": "/#deep-reinforcement-learning", 
            "text": "David Silver (Google DeepMind) - Deep Reinforcement Learning", 
            "title": "Deep Reinforcement Learning"
        }, 
        {
            "location": "/#recurrent-neural-networks", 
            "text": "A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. This makes them applicable to tasks such as unsegmented connected handwriting recognition or speech recognition", 
            "title": "Recurrent Neural Networks"
        }, 
        {
            "location": "/#1-applications-1", 
            "text": "", 
            "title": "1. Applications 1"
        }, 
        {
            "location": "/#11-sequence-to-sequence-language-translation-2", 
            "text": "", 
            "title": "1.1 Sequence-to-sequence language translation 2"
        }, 
        {
            "location": "/#12-generate-image-caption", 
            "text": "", 
            "title": "1.2 Generate image caption"
        }, 
        {
            "location": "/#13-translate-videos-to-sentences", 
            "text": "", 
            "title": "1.3 Translate videos to sentences"
        }, 
        {
            "location": "/#2-challenges", 
            "text": "Bag of Words Meets Bags of Popcorn", 
            "title": "2. Challenges"
        }, 
        {
            "location": "/#3-tutorials", 
            "text": "Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN)  The Unreasonable Effectiveness of Recurrent Neural Networks", 
            "title": "3. Tutorials"
        }, 
        {
            "location": "/#4-library", 
            "text": "neuraltalk2   (realtime demo)[https://vimeo.com/146492001]       Recurrent Neural Network    General Sequence Learning using Recurrent Neural Networks", 
            "title": "4. Library"
        }
    ]
}