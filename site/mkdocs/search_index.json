{
    "docs": [
        {
            "location": "/", 
            "text": "Deep Learning\n\n\n\n\nDeep learning (also known as deep structured learning, hierarchical learning or deep machine learning) is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using a deep graph with multiple processing layers, composed of multiple linear and non-linear ...\n\n\nBooks\n\n\n\n\n\n\n\n\nCourses\n\n\n\n\n\n\n\n\nVideos", 
            "title": "Home"
        }, 
        {
            "location": "/#deep-learning", 
            "text": "Deep learning (also known as deep structured learning, hierarchical learning or deep machine learning) is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using a deep graph with multiple processing layers, composed of multiple linear and non-linear ...", 
            "title": "Deep Learning"
        }, 
        {
            "location": "/#books", 
            "text": "", 
            "title": "Books"
        }, 
        {
            "location": "/#courses", 
            "text": "", 
            "title": "Courses"
        }, 
        {
            "location": "/#videos", 
            "text": "", 
            "title": "Videos"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Getting Started\n\n\nTo start with deep learning, you can choose either tensorflow, theano\n\n\nTensorflow\n\n\nWindows Installation\n\n\nIn this section, I will introduce how to setup your tensorflow library working with GPU in Windows.\n\n\nMy environment is:\n\n\n\n\nWindows 8.1 Pro 64-bit\n\n\nGraphic Card: NVIDIA GeForce GTX 980\n\n\n\n\nWhat are missing?\n\n\n\n\nAnaconda\n\n\nCUDA Toolkit 8.0 (cuda_8.0.44_windows.exe)\n\n\nCUDNN - CUDA for Deep Neural Networks (cudnn-8.0-windows7-x64-v5.1.zip)\n\n\ntensorflow\n\n\n\n\nAnaconda\n\n\nAnaconda is the leading open data science platform powered by Python. The open source version of Anaconda is a high performance distribution of Python and R and includes over 100 of the most popular Python, R and Scala packages for data science.\n\n\nStep 1: Download the \nAnaconda installer\n\n\nStep 2: Double click the Anaconda installer and follow the prompts to install to the default location.\n\n\nAfter a successful installation you will see output like this:\n\n\n\n\nCUDA Toolkit 8.0\n\n\nThe NVIDIA CUDA Toolkit provides a comprehensive development environment for C and C++ developers building GPU-accelerated applications. The CUDA Toolkit includes a compiler for NVIDIA GPUs, math libraries, and tools for debugging and optimizing the performance of your applications. You\u2019ll also find programming guides, user manuals, API reference, and other documentation to help you get started quickly accelerating your application with GPUs.\n\n\nStep 1: Verify the system has a CUDA-capable GPU.\n\n\nStep 2: Download the \nNVIDIA CUDA Toolkit\n.\n\n\nStep 3: Install the NVIDIA CUDA Toolkit.\n\n\nStep 4: Test that the installed software runs correctly and communicates with the hardware.\n\n\n\n\ncuDNN\n\n\nThe NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers. cuDNN is part of the NVIDIA Deep Learning SDK.\n\n\nStep 1: Register an NVIDIA developer account\n\n\nStep 2: Download \ncuDNN v5.1\n, you will get file like that \ncudnn-8.0-windows7-x64-v5.1.zip\n\n\n\n\nStep 3: Copy CUDNN files to CUDA install\n\n\nExtract your \ncudnn-8.0-windows7-x64-v5.1.zip\n file, and copy files to corresponding CUDA folder\n\n\nIn my environment, CUDA installed in \nC:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0\n, you must copy append three folders \nbin\n, \ninclude\n, \nlib\n\n\n\n\nTensorflow\n\n\nCPU TensorFlow environment\n\n\nconda create --name tensorflow python=3.5\nactivate tensorflow\nconda install -y jupyter scipy\npip install tensorflow\n\n\n\n\nGPU TensorFlow environment\n\n\nconda create --name tensorflow-gpu python=3.5\nactivate tensorflow-gpu\nconda install -y jupyter scipy\npip install tensorflow-gpu\n\n\n\n\nReferences\n\n\n\n\nUsing TensorFlow in Windows with a GPU, \nhttp://www.heatonresearch.com/2017/01/01/tensorflow-windows-gpu.html\n\n\nsentdex, Installing CPU and GPU TensorFlow on Windows, \nhttps://www.youtube.com/watch?v=r7-WPbx8VuY\n\n\n\n\nword2vec Example\n\n\nStep 1: Download word2vec example from \ngithub\n\n\n$ dir\n\n02/06/2017  11:45    \nDIR\n          .\n02/06/2017  11:45    \nDIR\n          ..\n02/06/2017  10:12             9,144 word2vec_basic.py\n\n\n\n\nStep 2: Run \nword2vec_basic\n example\n\n\n$ activate tensorflow-gpu\n$ python word2vec_basic.py\n\nFound and verified text8.zip\nData size 17005207\nMost common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\nSample data [5241, 3082, 12, 6, 195, 2, 3136, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n3082 originated -\n 5241 anarchism\n3082 originated -\n 12 as\n12 as -\n 6 a\n12 as -\n 3082 originated\n6 a -\n 195 term\n6 a -\n 12 as\n195 term -\n 2 of\n195 term -\n 6 a\nInitialized\nAverage loss at step  0 :  288.173675537\nNearest to its: nasl, tinkering, derivational, yachts, emigrated, fatalism, kingston, kochi,\nNearest to into: streetcars, neglecting, deutschlands, lecture, realignment, bligh, donau, medalists,\nNearest to state: canterbury, exceptions, disaffection, crete, westernmost, earthly, organize, richland,\n...", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#getting-started", 
            "text": "To start with deep learning, you can choose either tensorflow, theano", 
            "title": "Getting Started"
        }, 
        {
            "location": "/getting_started/#tensorflow", 
            "text": "", 
            "title": "Tensorflow"
        }, 
        {
            "location": "/getting_started/#windows-installation", 
            "text": "In this section, I will introduce how to setup your tensorflow library working with GPU in Windows.  My environment is:   Windows 8.1 Pro 64-bit  Graphic Card: NVIDIA GeForce GTX 980   What are missing?   Anaconda  CUDA Toolkit 8.0 (cuda_8.0.44_windows.exe)  CUDNN - CUDA for Deep Neural Networks (cudnn-8.0-windows7-x64-v5.1.zip)  tensorflow", 
            "title": "Windows Installation"
        }, 
        {
            "location": "/getting_started/#anaconda", 
            "text": "Anaconda is the leading open data science platform powered by Python. The open source version of Anaconda is a high performance distribution of Python and R and includes over 100 of the most popular Python, R and Scala packages for data science.  Step 1: Download the  Anaconda installer  Step 2: Double click the Anaconda installer and follow the prompts to install to the default location.  After a successful installation you will see output like this:", 
            "title": "Anaconda"
        }, 
        {
            "location": "/getting_started/#cuda-toolkit-80", 
            "text": "The NVIDIA CUDA Toolkit provides a comprehensive development environment for C and C++ developers building GPU-accelerated applications. The CUDA Toolkit includes a compiler for NVIDIA GPUs, math libraries, and tools for debugging and optimizing the performance of your applications. You\u2019ll also find programming guides, user manuals, API reference, and other documentation to help you get started quickly accelerating your application with GPUs.  Step 1: Verify the system has a CUDA-capable GPU.  Step 2: Download the  NVIDIA CUDA Toolkit .  Step 3: Install the NVIDIA CUDA Toolkit.  Step 4: Test that the installed software runs correctly and communicates with the hardware.", 
            "title": "CUDA Toolkit 8.0"
        }, 
        {
            "location": "/getting_started/#cudnn", 
            "text": "The NVIDIA CUDA Deep Neural Network library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers. cuDNN is part of the NVIDIA Deep Learning SDK.  Step 1: Register an NVIDIA developer account  Step 2: Download  cuDNN v5.1 , you will get file like that  cudnn-8.0-windows7-x64-v5.1.zip   Step 3: Copy CUDNN files to CUDA install  Extract your  cudnn-8.0-windows7-x64-v5.1.zip  file, and copy files to corresponding CUDA folder  In my environment, CUDA installed in  C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v8.0 , you must copy append three folders  bin ,  include ,  lib", 
            "title": "cuDNN"
        }, 
        {
            "location": "/getting_started/#tensorflow_1", 
            "text": "CPU TensorFlow environment  conda create --name tensorflow python=3.5\nactivate tensorflow\nconda install -y jupyter scipy\npip install tensorflow  GPU TensorFlow environment  conda create --name tensorflow-gpu python=3.5\nactivate tensorflow-gpu\nconda install -y jupyter scipy\npip install tensorflow-gpu  References   Using TensorFlow in Windows with a GPU,  http://www.heatonresearch.com/2017/01/01/tensorflow-windows-gpu.html  sentdex, Installing CPU and GPU TensorFlow on Windows,  https://www.youtube.com/watch?v=r7-WPbx8VuY", 
            "title": "Tensorflow"
        }, 
        {
            "location": "/getting_started/#word2vec-example", 
            "text": "Step 1: Download word2vec example from  github  $ dir\n\n02/06/2017  11:45     DIR           .\n02/06/2017  11:45     DIR           ..\n02/06/2017  10:12             9,144 word2vec_basic.py  Step 2: Run  word2vec_basic  example  $ activate tensorflow-gpu\n$ python word2vec_basic.py\n\nFound and verified text8.zip\nData size 17005207\nMost common words (+UNK) [['UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\nSample data [5241, 3082, 12, 6, 195, 2, 3136, 46, 59, 156] ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n3082 originated -  5241 anarchism\n3082 originated -  12 as\n12 as -  6 a\n12 as -  3082 originated\n6 a -  195 term\n6 a -  12 as\n195 term -  2 of\n195 term -  6 a\nInitialized\nAverage loss at step  0 :  288.173675537\nNearest to its: nasl, tinkering, derivational, yachts, emigrated, fatalism, kingston, kochi,\nNearest to into: streetcars, neglecting, deutschlands, lecture, realignment, bligh, donau, medalists,\nNearest to state: canterbury, exceptions, disaffection, crete, westernmost, earthly, organize, richland,\n...", 
            "title": "word2vec Example"
        }, 
        {
            "location": "/introduction/", 
            "text": "Modern Practical Deep Networks\n\n\n\n\nFeedforward Deep Networks\n\n\nRegularization\n\n\nOptimization for Training Deep Models\n\n\nConvolutional Networks\n\n\nSequence Modeling: Recurrent and Recursive Nets\n\n\nApplications\n\n\n\n\nDeep Learning Research\n\n\n\n\nStructured Probabilistic Models for Deep Learning\n\n\nMonte Carlo Methods\n\n\nLinear Factor Models and Auto-Encoders\n\n\nRepresentation Learning\n\n\nThe Manifold Perspective on Representation Learning\n\n\nConfronting the Partition Function\n\n\nApproximate Inference\n\n\nDeep Generative Models\n\n\n\n\nSlide\n\n\n\n\nAndrew Ng, Deep Learning, http://www.slideshare.net/mobile/ExtractConf/andrew-ng-chief-scientist-at-baidu\n\n\n\n\nCompare Deep Learning Framework\n\n\nhttps://github.com/zer0n/deepframeworks/blob/master/README.md\n\n\nPapers\n\n\nAwesome Deep Learning Papers\n\n\nDeep Learning Q\nA\n\n\nDeep Learning Q\nA \n1\n\n\n\n\nWhat is an auto-encoder? Why do we \"auto-encode\"? Hint: it's really a misnomer.\n\n\nWhat is a Boltzmann Machine? Why a Boltzmann Machine?\n\n\nWhy do we use sigmoid for an output function? Why tanh? Why not cosine? Why any function in particular?\n\n\nWhy are CNNs used primarily in imaging and not so much other tasks?\n\n\nExplain backpropagation. Seriously. To the target audience described above.\n\n\nIs it OK to connect from a Layer 4 output back to a Layer 2 input?\n\n\n\n\nA data-scientist person recently put up a YouTube video explaining that the essential difference between a Neural Network and a Deep Learning network is that the former is trained from output back to input, while the latter is trained from input toward output. Do you agree? Explain.\n\n\n\n\n\n\nCan you derive the back-propagation and weights update?\n\n\n\n\nExtend the above question to non-trivial layers such as convolutional layers, pooling layers, etc.\n\n\nHow to implement dropout\n\n\nYour intuition when and why some tricks such as max pooling, ReLU, maxout, etc. work. There are no right answers but it helps to understand your thoughts and research experience.\n\n\nCan youabstract the forward, backward, update operations as matrix operations, to leverage BLAS and GPU?\n\n\n\n\nIntro to Deep Learning\n\n\n\n\nWhat is deep learning?\n\n\n\nAccording to wikipedia \n1\n\n\nDeep learning\n \u00a0is a branch of machine learning based on a set of algorithms that \nattempt to model high-level abstractions\n in data by \nusing model architectures\n, with \ncomplex structures\n or otherwise, \ncomposed of multiple non-linear transformations.\n\n\nDeep learning is part of a broader family of machine learning methods based on \nlearning representations\n of data.\n\nAn observation (e.g., an image) can be represented in many ways such as a vector of intensity values per pixel, or in a more abstract way as a set of edges, regions of particular shape, etc.. Some representations make it easier to learn tasks (e.g., face recognition or facial expression recognition) from examples.\n\nOne of the promises of deep learning is replacing \nhandcrafted features\n with efficient algorithms for unsupervised or semi-supervised feature learning and hierarchical feature extraction.\n\n\n\nTraditional Model vs Deep Learning\n\n\n\nIn \ntraditional models\n, we must extract features by hand, after that we train these features with some classifiers\n\n\n\n\nWith \ndeep learning\n, we can learn representation of objects as well as its classifiers.\n\n\n\n\nHierarchy of representations with increasing level of abstraction. Each stage is a kind of trainable feature transform.\n\n\nImage recognition\n\n\npixel \n edge \n texton \n motif \n part \n object\n\n\nText\n\n\ncharacter \n word \n word group \n clause \n sentence \n story\n\n\nSpeech\n\n\nsample \n spectral band \n sound \n ... \n phone \n phoneme \n word\n\n\nDemos and Applications\n\n\n\nYann Lecun with ImageNetOnline Learning Demo in his deep learning class \n2\n.\u00a0This program auto learn new object when he pointed camera to it\n\n\n\n\nVoice recognition systems like Apple Siri, Google Now and Windows Cortana all use deep learning \n3\n\n\n\n\nFacebook's DeepFace Software Can Match Faces With 97.25% Accuracy \n4\n\n\n\n\nUseful Resources\n\n\n\n\n\nIntroduction about Deep Learning, http://colah.github.io/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u00a0\nDeep Learning, wikipedia\n\n\n\n\n\n\n\nDeep Learning Course of Yann Lecu, Week 1\n\n\n\n\n\n\n\nMeet The Guy Who Helped Google Beat Apple's Siri\n\n\n\n\n\n\n\u00a0\nFacebook's DeepFace Software Can Match Faces With 97.25% Accuracy\n\n\n\n\n\n\n\n\n\n\nTensorFlow\n\n\n\n\nTensorFlow\u2122 \n2\n is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.\n\n\nGetting Started\n\n\nLinux\n, \nMac\n, \nWindows\n\n\nWindows \n1\n\n\n[code lang=\"shell\"]\ndocker-machine ssh default\ndocker run -it b.gcr.io/tensorflow/tensorflow\n[/code]\n\n\nResources\n\n\n\n\nTensorFlow Explained by Jeff Dean (\nvideo\n)\n\n\n\n\nArchitectures\n\n\nAccording to Yann Lecun \n1\n, there are three types of deep architectures: feed-forward, feed-back and bi-directional.\n\n\nFeed-Forwards\n\n\n\nMultilayer Neural Nets \n2\n\n\n\nA multilayer perceptron (MLP) is a feedforward artificial neural network model that maps sets of input data onto a set of appropriate outputs. A MLP consists of multiple layers of nodes in a directed graph, with each layer fully connected to the next one. Except for the input nodes, each node is a neuron (or processing element) with a nonlinear activation function.\n\n\n\n\ntask: any supervised learning pattern recognition process\n\n\nConvolutional Neural Nets \n3\n\n\n\nIn machine learning, a convolutional neural network (CNN, or ConvNet) is a type of feed-forward artificial neural network where the individual neurons are tiled in such a way that they respond to overlapping regions in the visual field.Convolutional networks were inspired by biological processes and are variations of multilayer perceptrons which are designed to use minimal amounts of preprocessing. They are widely used models for image and video recognition.\n\n\n\n\ntask:\u00a0Computer Vision\n\n\nFeed-Back\n\n\n\nStacked sparse coding\n\n\n\nDeconvolutional nets\n\n\n\nBi-Directional\n\n\n\nRecurrent neural network \n4\n\n\n\nA recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior.\n\n\n\n\nTask: \nWord Embeddings\n\n\nDeep Boltzmann\u00a0Network\n\n\n\n\n\nStacked auto-encoders \n5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttp://www.slideshare.net/yandex/yann-le-cun\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Multilayer_perceptron\n\n\n\n\n\n\n\u00a0\nhttps://en.wikipedia.org/wiki/Convolutional_neural_network\n\n\n\n\n\n\n\nRecurrent neural network\n\n\n\n\n\n\n\nhttp://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders\n\n\n\n\n\n\n\n\n\n\nDeep Reinforcement Learning\n\n\nDavid Silver (Google DeepMind) - Deep Reinforcement Learning\n\n\nRecurrent Neural Networks\n\n\nA recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. This makes them applicable to tasks such as unsegmented connected handwriting recognition or speech recognition\n\n\n1. Applications \n1\n\n\n1.1 Sequence-to-sequence language translation \n2\n\n\n\n\n1.2 Generate image caption\n\n\n\n\n1.3 Translate videos to sentences\n\n\n\n\n2. Challenges\n\n\n\n\nBag of Words Meets Bags of Popcorn\n\n\n\n\n3. Tutorials\n\n\n\n\nAnyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN)\n\n\nThe Unreasonable Effectiveness of Recurrent Neural Networks\n\n\n\n\n4. Library\n\n\n\n\nneuraltalk2\n \n(realtime demo)[https://vimeo.com/146492001]\n\n\n\n\n\n\n\n\n\n\n\n\nRecurrent Neural Network\n\n\n\n\n\n\nGeneral Sequence Learning using Recurrent Neural Networks", 
            "title": "Introduction"
        }, 
        {
            "location": "/introduction/#modern-practical-deep-networks", 
            "text": "Feedforward Deep Networks  Regularization  Optimization for Training Deep Models  Convolutional Networks  Sequence Modeling: Recurrent and Recursive Nets  Applications", 
            "title": "Modern Practical Deep Networks"
        }, 
        {
            "location": "/introduction/#deep-learning-research", 
            "text": "Structured Probabilistic Models for Deep Learning  Monte Carlo Methods  Linear Factor Models and Auto-Encoders  Representation Learning  The Manifold Perspective on Representation Learning  Confronting the Partition Function  Approximate Inference  Deep Generative Models", 
            "title": "Deep Learning Research"
        }, 
        {
            "location": "/introduction/#slide", 
            "text": "Andrew Ng, Deep Learning, http://www.slideshare.net/mobile/ExtractConf/andrew-ng-chief-scientist-at-baidu   Compare Deep Learning Framework  https://github.com/zer0n/deepframeworks/blob/master/README.md", 
            "title": "Slide"
        }, 
        {
            "location": "/introduction/#papers", 
            "text": "Awesome Deep Learning Papers", 
            "title": "Papers"
        }, 
        {
            "location": "/introduction/#deep-learning-qa", 
            "text": "Deep Learning Q A  1   What is an auto-encoder? Why do we \"auto-encode\"? Hint: it's really a misnomer.  What is a Boltzmann Machine? Why a Boltzmann Machine?  Why do we use sigmoid for an output function? Why tanh? Why not cosine? Why any function in particular?  Why are CNNs used primarily in imaging and not so much other tasks?  Explain backpropagation. Seriously. To the target audience described above.  Is it OK to connect from a Layer 4 output back to a Layer 2 input?   A data-scientist person recently put up a YouTube video explaining that the essential difference between a Neural Network and a Deep Learning network is that the former is trained from output back to input, while the latter is trained from input toward output. Do you agree? Explain.    Can you derive the back-propagation and weights update?   Extend the above question to non-trivial layers such as convolutional layers, pooling layers, etc.  How to implement dropout  Your intuition when and why some tricks such as max pooling, ReLU, maxout, etc. work. There are no right answers but it helps to understand your thoughts and research experience.  Can youabstract the forward, backward, update operations as matrix operations, to leverage BLAS and GPU?", 
            "title": "Deep Learning Q&amp;A"
        }, 
        {
            "location": "/introduction/#intro-to-deep-learning", 
            "text": "", 
            "title": "Intro to Deep Learning"
        }, 
        {
            "location": "/introduction/#tensorflow", 
            "text": "TensorFlow\u2122  2  is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. TensorFlow was originally developed by researchers and engineers working on the Google Brain Team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research, but the system is general enough to be applicable in a wide variety of other domains as well.", 
            "title": "TensorFlow"
        }, 
        {
            "location": "/introduction/#getting-started", 
            "text": "Linux ,  Mac ,  Windows  Windows  1  [code lang=\"shell\"]\ndocker-machine ssh default\ndocker run -it b.gcr.io/tensorflow/tensorflow\n[/code]", 
            "title": "Getting Started"
        }, 
        {
            "location": "/introduction/#resources", 
            "text": "TensorFlow Explained by Jeff Dean ( video )", 
            "title": "Resources"
        }, 
        {
            "location": "/introduction/#architectures", 
            "text": "According to Yann Lecun  1 , there are three types of deep architectures: feed-forward, feed-back and bi-directional.", 
            "title": "Architectures"
        }, 
        {
            "location": "/introduction/#deep-reinforcement-learning", 
            "text": "David Silver (Google DeepMind) - Deep Reinforcement Learning", 
            "title": "Deep Reinforcement Learning"
        }, 
        {
            "location": "/introduction/#recurrent-neural-networks", 
            "text": "A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. This makes them applicable to tasks such as unsegmented connected handwriting recognition or speech recognition", 
            "title": "Recurrent Neural Networks"
        }, 
        {
            "location": "/introduction/#1-applications-1", 
            "text": "", 
            "title": "1. Applications 1"
        }, 
        {
            "location": "/introduction/#11-sequence-to-sequence-language-translation-2", 
            "text": "", 
            "title": "1.1 Sequence-to-sequence language translation 2"
        }, 
        {
            "location": "/introduction/#12-generate-image-caption", 
            "text": "", 
            "title": "1.2 Generate image caption"
        }, 
        {
            "location": "/introduction/#13-translate-videos-to-sentences", 
            "text": "", 
            "title": "1.3 Translate videos to sentences"
        }, 
        {
            "location": "/introduction/#2-challenges", 
            "text": "Bag of Words Meets Bags of Popcorn", 
            "title": "2. Challenges"
        }, 
        {
            "location": "/introduction/#3-tutorials", 
            "text": "Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN)  The Unreasonable Effectiveness of Recurrent Neural Networks", 
            "title": "3. Tutorials"
        }, 
        {
            "location": "/introduction/#4-library", 
            "text": "neuraltalk2   (realtime demo)[https://vimeo.com/146492001]       Recurrent Neural Network    General Sequence Learning using Recurrent Neural Networks", 
            "title": "4. Library"
        }, 
        {
            "location": "/cnn/", 
            "text": "Convolutional Neural Networks\n\n\nConvolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply.\n\n\nSo what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.", 
            "title": "CNN"
        }, 
        {
            "location": "/cnn/#convolutional-neural-networks", 
            "text": "Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply.  So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.", 
            "title": "Convolutional Neural Networks"
        }
    ]
}